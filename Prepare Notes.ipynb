{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# datetime utilities\n",
    "from datetime import timedelta, datetime\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# no yelling in the library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# our acquire script\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = acquire.stores_df()\n",
    "stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items= acquire.items_df()\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = acquire.sales_df()\n",
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column to be merged on\n",
    "sales = sales.rename(columns={'item':'item_id'})\n",
    "#merge sales and items on 'item_id'\n",
    "merged_df = pd.merge(sales, items, on=\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column to be merged on\n",
    "stores = stores.rename(columns={'store_id':'store'})\n",
    "#merge stores to already merged df on store column\n",
    "complete_df = pd.merge(merged_df, stores, on=\"store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "strftime('%a, %d, %b %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sale_date = pd.to_datetime(df.sale_date, format='%a, %d %b %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dtypes\n",
    "complete_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must complete step 2&3 (set date as index and sort)\n",
    "complete_df = complete_df.set_index('sale_date').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure datatime is index now\n",
    "type(complete_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.shape[0], complete_df.sale_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this shows that there are distinct paterns in this data\n",
    "#we have peaks and valleys depending on season\n",
    "#market seems predictable\n",
    "by_date = df.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "by_date.plot(x='sale_date', y='sale_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate info with even distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations per store = number of item-transactions per store.\n",
    "df.store.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of items\n",
    "df.item_id.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "This all shows that there is an even distribution of items and stores within the dataframe\n",
    "- 50 unique items\n",
    "- 18260 sales per item\n",
    "- 10 unique stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out what happens on first day of sales\n",
    "#remove the index of datetime so we can use it in groupby (only for this problem)\n",
    "first_sale = df.reset_index().groupby(['store', 'item_id']).sale_date.min()\n",
    "first_sale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique dates from first_sale_per_storeitem\n",
    "first_sale.unique() # if there's only one unique value, then YES!\n",
    "\n",
    "#shows that every item was sold in everystore on the that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at last sale\n",
    "last_sale = df.reset_index().groupby(['store', 'item_id']).sale_date.max()\n",
    "last_sale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sale.unique() # if there's only one unique value, then YES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see all the DIFFERENT days that items were sold at each store\n",
    "days_per_store_per_item =  df.reset_index().groupby(['store', 'item_id']).sale_date.nunique()\n",
    "days_per_store_per_item.head()\n",
    "\n",
    "#there are no unique days.\n",
    "#every item was sold every day at every store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can see that sale_date is still index within the original df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for time gaps in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this shows that there are no missing days or values\n",
    "#don't need to fill or pad this data\n",
    "print('Number of rows:', df.index.nunique())\n",
    "n_days = df.index.max() - df.index.min() + pd.Timedelta('1d')\n",
    "print(f\"Number of days between first and last day:\", n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_store_data(df):\n",
    "    return df.asign(sale_date=pd.to_datetime(df.sale_date)).sort_values('sale_date').set_index('sale_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "- **sklearn.model_selection.TimeSeriesSplit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting time series data into train, test, and validate sets is a little trickier than with previous data we have looked at. Because the data points have an order to them, we cannot simply assign each point randomly to train, validate, or test.\n",
    "- Ideally all splits should contain one season's worth of data. There are several methods we can use to split our time series data:\n",
    "    - Human-based: use, for example, the last year in the dataset as test split\n",
    "    - Percentage based: use the last 20% as test\n",
    "    - Cross Validate: break data up into slices and use successive slices as train and test repeatedly (sklearn.model_selection.TimeSeriesSplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important:\n",
    "- We have to cut at a specific point in time\n",
    "- cannot just scramble data up like before\n",
    "- make the cut based on a cutoff on train that accurately sees seasonality\n",
    "- overfitting can happen but to avoid it we must draw lines within validate and split it into subsets (**crossvalidation**)\n",
    "    - K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
